{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploring Deep Learning to Develop Automatic Essay Scoring Models**\n",
    "--------------------------------------------------------------------------------\n",
    "## **Purpose of this project**\n",
    "\n",
    "### Nowadays, students are studing on different online learning platforms. These platforms also provide assessments to show learners' progress. Multiple-choice questions are easy to be grade because there are correct answers. However, open-ended questions or essays are harder to grade. Peer review can be designed as an activity. When human graders are involved, they need to spend a large amout of time first reading the responses and then giving the score. Many education companies are trying to develop automatic essay scoring models, but encounter some problems in model development or implementation. Fortunately, we now have deep learning models and pre-trained large language models. This project will try to develop an automatic essay scoring model through deep learning.\n",
    "--------------------------------------------------------------------------------\n",
    "## **Goal of this project**\n",
    "\n",
    "### The aim of this project is to use different deep learning models and a pre-trained large language model to develop an automatic essay scoring model. This is supervised multi-classification problem because the labels are provided and the labels are 1, 2, 3, 4, 5, 6. In the project, I will use a simple deep learning model (a multilayer perceptron), three sequential neural networks (bi-RNN, bi-LSTM, bi-GRU), and a pre-trained large language model (DeBERTa-V3-Base), and then compare their performance.\n",
    "--------------------------------------------------------------------------------\n",
    "## **Dataset**\n",
    "\n",
    "### The data is from https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/data. The competition has been closed. I selected the training data and just used the training data to develop different models. This dataset consists of 17307 observations and 3 variables (*essay_id*, *full_text*, and *score*). The variable of \"full_text\" is about students' essays and score is about their essay scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/content/drive/MyDrive/train_automatic_essay_scoring.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory data analysis**\n",
    "### I first checked whether there were missing values in the text and score columns. No missing values. Then, I used histogram to visualize the score variable. We could see there was huge imbalance of data in this dataset which could post a big challenge to the model performance. There are over 6000 essays with a score of 3, over 4000 essays with a score of 2, around 4000 essays with a score of 4. There are just over 1000 essays each for score 1 and 5. The highest score, 6, is lowest, with around 1000 essays. My hypothesis is that the model would achieve a better performance in scores like 2, 3, 4 but have a low performance in score 6. Or possibly, the model could not predict successfully in essays with score 6.\n",
    "\n",
    "### Here I would like to add two examples to show the essay text and its corresponding score.\n",
    "\n",
    "### **An essay with a score of 3**\n",
    "\n",
    "*  Score 3 \"I am a scientist at NASA that is discussing the \"face\" on mars. I will be explaining how the \"face\" is a land form. By sharing my information about this isue i will tell you just that.\n",
    "\n",
    "   First off, how could it be a martions drawing. There is no plant life on mars as of rite now that we know of, which means so far as we know it is not possible for any type of life. That explains how it could not be made by martians. Also why and how would a martion build a face so big. It just does not make any since that a martian did this.\n",
    "\n",
    "   Next, why it is a landform. There are many landforms that are weird here in America, and there is also landforms all around the whole Earth. Many of them look like something we can relate to like a snake a turtle a human... So if there are landforms on earth dont you think landforms are on mars to? Of course! why not? It's just unique that the landform on Mars looks like a human face. Also if there was martians and they were trying to get our attention dont you think we would have saw one by now?\n",
    "\n",
    "   Finaly, why you should listen to me. You should listen to me because i am a member of NASA and i've been dealing with all of this stuff that were talking about and people who say martians did this have no relation with NASA and have never worked with anything to relate to this landform. One last thing is that everyone working at NASA says the same thing i say, that the \"face\" is just a landform.\n",
    "\n",
    "   To sum all this up the \"face\" on mars is a landform but others would like to beleive it's a martian sculpture. Which every one that works at NASA says it's a landform and they are all the ones working on the planet and taking pictures.\"\n",
    "\n",
    "### **Another essay with a score of 6**\n",
    "\n",
    "*   Score 6: Sometimes.\" (Plumer Paragraph 10) What the author is explaining is that during the presidential election, once you vote on your selection for the next president and you give your vote to the state electors you never know if they might change their mind or get scared and choose the incorrect candidate. For example, you choose on Barack Obama for president and you give your vote to the state electors and when it's time to vote... they decide to switch and choose HILARY CLINTON! Many of the citizens who voted for Barack Obama are now outraged by the thought of their state electors doing such a thing. If we had elections by popular vote we would be able to choose whom we specifically want for our President and there wouldn't be so much tension between people   \n",
    "\n",
    "    Furthermore, the article \"The indefensible electoral college: Why even the best-laid defenses of the system ae wrong\" Bradford explains \"Back in 1960, Segregationists in the Louisinna legislaure nearly succeeded in replacing the Democratic electors with new electors whoo would oppose John F. Kennedy.\" This quote from the article is saying that the electors could easily manipulate you and change their votes in order to get what they want, forgetting about all the other votes of the people back home waiting for the news that their selection has won the presidency.The elecoral college completely demolishes the purpose of the people's vote.\n",
    "\n",
    "    Additionally, electoral colleges should be abolished because not everyone feels as strongly about it as they did hundreds of years ago when the process first came about. What had started out as a good idea has slowly turned into a unpredictable disaster. From time to time, People would be let down when they find out thatÂ  the candidate they had chosen didn't win the election, Why? because their state electors decided that it was okay for them to simply go against everyone else and be selfish by choosing their own candidate for presidency.\n",
    "\n",
    "    Bradford proves this by explaining \"...'faithless' electors have occasionally refused to vote for their party's candidate and cast a deciding vote for whomever they please...\" (Plumer Paragraph 11) On multiple occasions voters have done exactly that, choosing someone completely different than whom they were supposed to. Many members of the party get angry with such childish behavior because it's selfish, uncalled for, and just disrespectful to go about ignoring the one major duty they had to cast a vote for their selected candidate. The article \"In defense of the electoral college: Five reasons to keep our despised method of choosing the president\" Richard A. Posner exclaims \"The electoral college is widely regarded as a anachronism, a non-democratic method of selecting a president that ought to be [overruled] by declaring the candidate who recieves the most popular votes the winner.\" (Posner Paragraph 15) What the author is explaining is that the electoral college is an old custom and it's time that it was changed to something new like the election by popular vote.\n",
    "\n",
    "    Time has changed, an so has the political veiws. The election by popular vote is a better opportunity because the state's people get to vote on exactly who they want without any major risks to deal with later on. Also, the election by popular vote is a simple and easier way of electing president.\n",
    "\n",
    "    On the other hand, there are very few reasons that are pointing towards the electoral college being a good idea. For example, The electoral college has a even number of votes which make it easier to have a more predictable outcome of who might win the election. Although, not everyone might get the candidate that they had hoped for originally. The electoral college also comes along with the \"Winner-take-all\" method in which the awarding electoral votes induces the candidates running for the presidency. However, this is only based on the candidate that has the most popular votes. There are various reasons to consider the electoral college but many of them are followed by an overload of reasons NOT to keep the electoral college in use.\n",
    "\n",
    "    Lastly, the election by popular vote should be used instead of the electoral college. The electoral college comes along with many complications and difficulties unlike the election by popular vote it has a simple and easier way of choosing who you want in the next presidency. Many people feel that you should change over to the election by popular vote to benefit all of the state's people so that they can have a more acurrate estimation of who theyÂ  might have as their new president.\n",
    "\n",
    "    According to Bradford, the electoral college is \"...Unfair, outdated, and irrational.\" (Plumer paragraph 14) It's about time we got rid of it and changed the way we elected our new president.\n",
    "\n",
    "### These essays are discussing different topics. The essay with a score of 6 obviously had a better argumentation, language organization and sophisticated thinking.\n",
    "\n",
    "## **Feature Engineering**\n",
    "### I engineered six basic variables to represent the quality of essays: text_length, word_count, sentence_count, spelling_errors, readability_score (i.e., whether the text is easy to read and understand), and grade_level (i.e., which grade is the text suitable for). I then created a correlation analysis and remove readability_score and grade_level because of low correlation with the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['full_text', 'score']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['score'], bins=20, edgecolor='black')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker    # create a new variable counting the number of spelling errors in the essay\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def count_spelling_errors(text):\n",
    "    words = text.lower().split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    return len(misspelled)\n",
    "\n",
    "df['spelling_errors'] = df['full_text'].apply(count_spelling_errors)\n",
    "print(df[['full_text', 'spelling_errors']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re     ## create a clean version of text but keep punctuation and grammar\n",
    "\n",
    "def basic_clean(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # normalize whitespace\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['text_clean'] = df['full_text'].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['text_norm'] = df['text_clean'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "def get_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def get_words(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['sentences'] = df['text_clean'].apply(get_sentences)\n",
    "df['words']     = df['text_norm'].apply(get_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length_clean']    = df['text_clean'].str.len()\n",
    "df['word_count_clean']     = df['words'].apply(len)\n",
    "df['sentence_count_clean'] = df['sentences'].apply(len).replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat\n",
    "import textstat\n",
    "\n",
    "df['readability_score'] = df['text_clean'].apply(textstat.flesch_reading_ease)\n",
    "df['grade_level']       = df['text_clean'].apply(textstat.flesch_kincaid_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = df[['text_length_clean', 'word_count_clean', 'sentence_count_clean', 'spelling_errors', 'readability_score', 'grade_level', 'score']].corr()\n",
    "print(corr)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Among Essay Features and Scores\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "\n",
    "*   I first used the four engineered variables to build a 3-layer multilayer perceptron model to predict the final score. I splitted the data 80% for training, 10% for validation, and 10% for testing. For this model, I chose ReLu activation and 0.3 for dropout. The first hidden layer had 64 neurons, the second hidden layer had 32 neurons, and the third hidden layer had 16 neurons. I selected cross entropy loss, adam optimizer and 50 epochs. Here I actually did some hyperparameter tuning, trying different learning rates and different epochs. I chose this model because I engineered four variables and attempted to use this model as the baseline model.\n",
    "\n",
    "*   Then, I built a sequential neural network pipeline (bi_RNN, bi_LSTM, bi_GRU). I chose these models because these essays are in long sequences and these models can better capture the temporal flow of information. I directly chose bidirectional because I think essay meaning is often influenced by both previous and upcoming context, and reading sequences in both forward and backward directions can help the model better understand relationships across the entire text. Before building models, I first created a word embedding with all the vocabularies in the text and gave them index. I used padding for those less than 300 words and unknown words. I implemented a unified recurrent model framework, RNNFamilyClassifier, which supports RNN, LSTM, and GRU architectures within a single pipeline. The model consists of an embedding layer, one recurrent layers (unidirectional or bidirectional), and a fully connected classification layer. Depending on the rnn_type parameter, the model dynamically initializes an RNN, LSTM, or GRU cell. Bidirectionality and depth are configurable, allowing the network to capture contextual information from both previous and upcoming tokens. The final prediction is obtained by concatenating the last forward and backward hidden states (if bidirectional) and passing them through a linear layer for classification.\n",
    "\n",
    "*   I finally used \"microsoft/deberta-v3-base\" as the primary model for fine-tuning, leveraging its pretrained language representations for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple neural nework as baseline model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = [\n",
    "    'text_length_clean',\n",
    "    'word_count_clean',\n",
    "    'sentence_count_clean',\n",
    "    'spelling_errors'\n",
    "]\n",
    "\n",
    "X = df[features].values.astype(np.float32)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['score'].values)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "# First split: hold out 20% as test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# so final proportions ~80% train, 10% val, 10% test\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val   = torch.tensor(y_val,   dtype=torch.long)\n",
    "y_test  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset   = TensorDataset(X_val,   y_val)\n",
    "test_dataset  = TensorDataset(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1=64, hidden2=32, hidden3=16, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden2, hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden3, num_classes)   # final classification layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = ThreeLayerMLP(input_dim=input_dim,\n",
    "                      hidden1=64, hidden2=32, hidden3=16,\n",
    "                      num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct, total, running_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            xb = xb.float()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        xb = xb.float()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_correct += (preds == yb).sum().item()\n",
    "        train_total += yb.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / train_total\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "\n",
    "    # Save best model based on validation accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "# Load best validation model\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    print(f\"\\nLoaded best model with val acc = {best_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(epochs, val_accs, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# gather predictions\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        xb = xb.float()\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric labels to strings for printing\n",
    "labels = le.classes_\n",
    "label_names = [str(x) for x in labels]   # <-- important fix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_names))\n",
    "\n",
    "# Confusion matrix (numbers)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Simple tokenizer\n",
    "def tokenize(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "texts = df[\"full_text\"].astype(str).tolist()\n",
    "tokenized_texts = [tokenize(t) for t in texts]\n",
    "\n",
    "# 2) Build vocab with min frequency\n",
    "min_freq = 5\n",
    "counter = Counter(word for doc in tokenized_texts for word in doc)\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "vocab = {\n",
    "    PAD_TOKEN: 0,\n",
    "    UNK_TOKEN: 1,\n",
    "}\n",
    "for word, freq in counter.items():\n",
    "    if freq >= min_freq:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "pad_id = vocab[PAD_TOKEN]\n",
    "unk_id = vocab[UNK_TOKEN]\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300  # you can adjust\n",
    "\n",
    "def encode(tokens):\n",
    "    ids = [vocab.get(tok, unk_id) for tok in tokens]\n",
    "    ids = ids[:MAX_LEN]\n",
    "    ids += [pad_id] * (MAX_LEN - len(ids))\n",
    "    return ids\n",
    "\n",
    "encoded_sequences = [encode(toks) for toks in tokenized_texts]\n",
    "encoded_sequences = np.array(encoded_sequences, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"score\"].values)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "X = encoded_sequences\n",
    "\n",
    "# 80% train, 10% val, 10% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset   = TextDataset(X_val,   y_val)\n",
    "test_dataset  = TextDataset(X_test,  y_test)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNFamilyClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_classes,\n",
    "        pad_id,\n",
    "        rnn_type=\"rnn\",\n",
    "        num_layers=1,\n",
    "        bidirectional=False,\n",
    "        dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "\n",
    "        rnn_cls = {\n",
    "            \"rnn\": nn.RNN,\n",
    "            \"lstm\": nn.LSTM,\n",
    "            \"gru\": nn.GRU\n",
    "        }[rnn_type]\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        factor = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_dim * factor, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, hidden = self.rnn(emb)\n",
    "\n",
    "        if self.rnn_type == \"lstm\":\n",
    "            h, c = hidden\n",
    "        else:\n",
    "            h = hidden\n",
    "\n",
    "\n",
    "        h = h.view(self.num_layers, self.num_directions, h.size(1), self.hidden_dim)\n",
    "\n",
    "\n",
    "        if self.bidirectional:\n",
    "            last_hidden = torch.cat(\n",
    "                (h[-1, 0], h[-1, 1]), dim=1\n",
    "            )\n",
    "        else:\n",
    "            last_hidden = h[-1, 0]\n",
    "\n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "            total_samples += yb.size(0)\n",
    "    return total_loss/total_samples, total_correct/total_samples\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss, total_correct, total_samples = 0,0,0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "            total_samples += yb.size(0)\n",
    "\n",
    "        train_loss = total_loss/total_samples\n",
    "        train_acc  = total_correct/total_samples\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"\\nLoaded Best Model — Val Acc = {best_val_acc:.3f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 200\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "\n",
    "# RNN\n",
    "rnn_model = RNNFamilyClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    pad_id=pad_id,\n",
    "    rnn_type=\"rnn\",\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "print(\"\\nTraining RNN...\")\n",
    "rnn_model = train_model(rnn_model, train_loader, val_loader, epochs=20, lr=1e-3)\n",
    "rnn_test_loss, rnn_test_acc = evaluate(rnn_model, test_loader)\n",
    "print(f\"RNN Test Loss: {rnn_test_loss:.4f}, Test Acc: {rnn_test_acc:.3f}\")\n",
    "\n",
    "# LSTM\n",
    "lstm_model = RNNFamilyClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    pad_id=pad_id,\n",
    "    rnn_type=\"lstm\",\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "print(\"\\nTraining LSTM...\")\n",
    "lstm_model = train_model(lstm_model, train_loader, val_loader, epochs=20, lr=1e-3)\n",
    "lstm_test_loss, lstm_test_acc = evaluate(lstm_model, test_loader)\n",
    "print(f\"LSTM Test Loss: {lstm_test_loss:.4f}, Test Acc: {lstm_test_acc:.3f}\")\n",
    "\n",
    "# GRU\n",
    "gru_model = RNNFamilyClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    pad_id=pad_id,\n",
    "    rnn_type=\"gru\",\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "print(\"\\nTraining GRU...\")\n",
    "gru_model = train_model(gru_model, train_loader, val_loader, epochs=20, lr=1e-3)\n",
    "gru_test_loss, gru_test_acc = evaluate(gru_model, test_loader)\n",
    "print(f\"GRU Test Loss: {gru_test_loss:.4f}, Test Acc: {gru_test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(yb.cpu().numpy())\n",
    "    return np.array(preds), np.array(labels)\n",
    "\n",
    "# Collect predictions for all 3 models\n",
    "rnn_preds, rnn_labels   = get_predictions(rnn_model,  test_loader)\n",
    "lstm_preds, lstm_labels = get_predictions(lstm_model, test_loader)\n",
    "gru_preds, gru_labels   = get_predictions(gru_model,  test_loader)\n",
    "\n",
    "# Build confusion matrices\n",
    "cm_rnn  = confusion_matrix(rnn_labels,  rnn_preds)\n",
    "cm_lstm = confusion_matrix(lstm_labels, lstm_preds)\n",
    "cm_gru  = confusion_matrix(gru_labels,  gru_preds)\n",
    "\n",
    "label_names = [str(x) for x in le.classes_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "def show_table(cm, name):\n",
    "    print(f\"\\n===== {name} Confusion Table =====\")\n",
    "    print(cm)\n",
    "\n",
    "show_table(cm_rnn,  \"RNN\")\n",
    "show_table(cm_lstm, \"LSTM\")\n",
    "show_table(cm_gru,  \"GRU\")\n",
    "\n",
    "print(\"===== RNN Classification Report =====\")\n",
    "print(\"Accuracy:\", accuracy_score(rnn_labels, rnn_preds))\n",
    "print(classification_report(rnn_labels, rnn_preds, target_names=label_names))\n",
    "\n",
    "print(\"\\n===== LSTM Classification Report =====\")\n",
    "print(\"Accuracy:\", accuracy_score(lstm_labels, lstm_preds))\n",
    "print(classification_report(lstm_labels, lstm_preds, target_names=label_names))\n",
    "\n",
    "print(\"\\n===== GRU Classification Report =====\")\n",
    "print(\"Accuracy:\", accuracy_score(gru_labels, gru_preds))\n",
    "print(classification_report(gru_labels, gru_preds, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"full_text\", \"score\"]).reset_index(drop=True)\n",
    "df[\"score\"] = df[\"score\"].astype(int)\n",
    "\n",
    "# Map scores 1–6 → 0–5 for the model\n",
    "df[\"label\"] = df[\"score\"] - 1\n",
    "df[[\"full_text\", \"score\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"full_text\", \"label\"]])\n",
    "\n",
    "temp = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "test_ds = temp[\"test\"]\n",
    "rest_ds = temp[\"train\"]\n",
    "\n",
    "splits = rest_ds.train_test_split(test_size=1/9, seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=6,   # <<< 6 classes: internal labels 0–5\n",
    "    problem_type=\"single_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"full_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_batch, batched=True)\n",
    "val_tok   = val_ds.map(tokenize_batch,   batched=True)\n",
    "test_tok  = test_ds.map(tokenize_batch,  batched=True)\n",
    "\n",
    "for ds in (train_tok, val_tok, test_tok):\n",
    "    ds = ds.rename_column(\"label\", \"labels\")\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"weighted_f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deberta-essay\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "\n",
    "    # OLD VERSION — REPLACEMENT SETTINGS\n",
    "    eval_steps=500,      # evaluate every 500 steps\n",
    "    save_steps=500,      # save every 500 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,      # VALIDATION STILL WORKS!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = trainer.predict(test_tok)\n",
    "raw_preds = pred_output.predictions\n",
    "\n",
    "y_true = pred_output.label_ids\n",
    "y_pred = raw_preds.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "labels = sorted(list(set(y_true)))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model results and analysis**\n",
    "### I fitted the models with some hyperparameter tuning, such as adding a new layer, changing the learning and increasing the number of epochs. Here is a summary of results:\n",
    "\n",
    "| Model | Type | Overall ACC | Note |\n",
    "|------|------|------------|--------|\n",
    "| MLP | Feedforward | 0.46 | Did not predict score 1, 5, and 6 |\n",
    "| Bi_RNN | Sequential | 0.46 | Performed worse in score 1; did not predict score 5 and 6 |\n",
    "| Bi_LSTM | Sequential | 0.50 | Performed worse in score 1 and 5; did not predict score 6 |\n",
    "| Bi_GRU | Sequential | 0.51 | Performed worse in score 1, 5, and 6 |\n",
    "| DeBERTa-V3-Base | Transformer  |   0.65     |   Performed worse in score 5; did not predict score 6 |\n",
    "\n",
    "### When I just used four engineered variables to predict the outcomes, the result was not good. The model almost could not predict any essays in score 1, 5, and 6. With sequential neural networks, the performance improved a little bit, but these models still struggled with essays with score 1, 5, and 6. The transformer DeBERTa-V3-Base improved the performance a lot. Now, it only struggled with essays with score 5, and 6. It actually had an average performance in score 5 but did not predict any essays in score 6. The main reason behind this is that there is a huge data imbalance. The number of essays in score 6 is pretty low. Even the transformer learned better and improve a lot, it still struggled with this.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "## **Learning and takeaways**\n",
    "\n",
    "*   Transformers had a better performance than sequential neural network models and feedforward neural models considering it is better at capturing the contextual and temporal information.\n",
    "*   If there is a huge data imbalance, it is hard to build a model with a good performance because it is hard for the model to capture information from unrepresented categories.\n",
    "*   Although the final performance may not reach to a satisfactory score, it still shows a decent progress.\n",
    "\n",
    "## **Ways to improve**\n",
    "\n",
    "*   Using the transformer is the recommended choice. But some new features need to be engineered especially the unique features of score 6. For example, whether score 6 produced longer text; whether score 6 included more transition words to structure the text; whether score 6 presented a more persuasive style\n",
    "*   Some data augmentation methods can be used. For example, consider using large language models to preduce responses in score 1, 5, and 6 to make the data more balanced. Still some unique features of essays of score 1, 5, and 6 should be distinguished first.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
